{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BBmnRsV__0ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required Packages\n",
        "!pip install torch numpy transformers datasets matplotlib tqdm scikit-learn openai dotmap"
      ],
      "metadata": {
        "id": "AalvnSOmT8tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import datasets\n",
        "import transformers\n",
        "import re\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tqdm\n",
        "import random\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, auc,  f1_score, accuracy_score, confusion_matrix, precision_score, recall_score\n",
        "import argparse\n",
        "import datetime\n",
        "import os\n",
        "import json\n",
        "import functools\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import time\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from dotmap import DotMap\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# 15 colorblind-friendly colors\n",
        "COLORS = [\"#0072B2\", \"#009E73\", \"#D55E00\", \"#CC79A7\", \"#F0E442\",\n",
        "            \"#56B4E9\", \"#E69F00\", \"#000000\", \"#0072B2\", \"#009E73\",\n",
        "            \"#D55E00\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\", \"#E69F00\"]\n",
        "\n",
        "# define regex to match all <extra_id_*> tokens, where * is an integer\n",
        "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
        "\n",
        "\n",
        "def load_base_model():\n",
        "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "    try:\n",
        "        mask_model.cpu()\n",
        "    except NameError:\n",
        "        pass\n",
        "    if args.openai_model is None:\n",
        "        base_model.to(DEVICE)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "\n",
        "\n",
        "def load_mask_model():\n",
        "    print('MOVING MASK MODEL TO GPU...', end='', flush=True)\n",
        "    start = time.time()\n",
        "\n",
        "    if args.openai_model is None:\n",
        "        base_model.cpu()\n",
        "    if not args.random_fills:\n",
        "        mask_model.to(DEVICE)\n",
        "    print(f'DONE ({time.time() - start:.2f}s)')\n",
        "\n",
        "\n",
        "def tokenize_and_mask(text, span_length, pct, ceil_pct=False):\n",
        "    tokens = text.split(' ')\n",
        "    mask_string = '<<<mask>>>'\n",
        "\n",
        "    n_spans = pct * len(tokens) / (span_length + args.buffer_size * 2)\n",
        "    if ceil_pct:\n",
        "        n_spans = np.ceil(n_spans)\n",
        "    n_spans = int(n_spans)\n",
        "\n",
        "    n_masks = 0\n",
        "    while n_masks < n_spans:\n",
        "        start = np.random.randint(0, len(tokens) - span_length)\n",
        "        end = start + span_length\n",
        "        search_start = max(0, start - args.buffer_size)\n",
        "        search_end = min(len(tokens), end + args.buffer_size)\n",
        "        if mask_string not in tokens[search_start:search_end]:\n",
        "            tokens[start:end] = [mask_string]\n",
        "            n_masks += 1\n",
        "\n",
        "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
        "    num_filled = 0\n",
        "    for idx, token in enumerate(tokens):\n",
        "        if token == mask_string:\n",
        "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
        "            num_filled += 1\n",
        "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def count_masks(texts):\n",
        "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
        "\n",
        "\n",
        "# replace each masked span with a sample from T5 mask_model\n",
        "def replace_masks(texts):\n",
        "    n_expected = count_masks(texts)\n",
        "    stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
        "    tokens = mask_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "    outputs = mask_model.generate(**tokens, max_length=150, do_sample=True, top_p=args.mask_top_p, num_return_sequences=1, eos_token_id=stop_id)\n",
        "    return mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def extract_fills(texts):\n",
        "    # remove <pad> from beginning of each text\n",
        "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
        "\n",
        "    # return the text in between each matched mask token\n",
        "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
        "\n",
        "    # remove whitespace around each fill\n",
        "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
        "\n",
        "    return extracted_fills\n",
        "\n",
        "\n",
        "def apply_extracted_fills(masked_texts, extracted_fills):\n",
        "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
        "    tokens = [x.split(' ') for x in masked_texts]\n",
        "\n",
        "    n_expected = count_masks(masked_texts)\n",
        "\n",
        "    # replace each mask token with the corresponding fill\n",
        "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
        "        if len(fills) < n:\n",
        "            tokens[idx] = text\n",
        "        else:\n",
        "            for fill_idx in range(n):\n",
        "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
        "\n",
        "    # join tokens back into text\n",
        "    texts = [\" \".join(x) for x in tokens]\n",
        "    return texts\n",
        "\n",
        "\n",
        "def perturb_texts_(texts, span_length, pct, ceil_pct=False):\n",
        "    if not args.random_fills:\n",
        "        masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
        "\n",
        "        raw_fills = replace_masks(masked_texts)\n",
        "        extracted_fills = extract_fills(raw_fills)\n",
        "        perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
        "\n",
        "\n",
        "        # Handle the fact that sometimes the model doesn't generate the right number of fills and we have to try again\n",
        "        attempts = 1\n",
        "        while '' in perturbed_texts:\n",
        "            #print(masked_texts)\n",
        "            #print(extracted_fills)\n",
        "            #print(perturbed_texts)\n",
        "            idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
        "            #print(idxs)\n",
        "            print(f'WARNING: {len(idxs)} texts have no fills. Trying again [attempt {attempts}].')\n",
        "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for idx, x in enumerate(texts) if idx in idxs]\n",
        "            raw_fills = replace_masks(masked_texts)\n",
        "            extracted_fills = extract_fills(raw_fills)\n",
        "            new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
        "            for idx, x in zip(idxs, new_perturbed_texts):\n",
        "                perturbed_texts[idx] = x\n",
        "            attempts += 1\n",
        "            if attempts == 10:\n",
        "                for idx, x in zip(idxs, new_perturbed_texts):\n",
        "                    perturbed_texts[idx] = 'hello'\n",
        "                break\n",
        "    else:\n",
        "        if args.random_fills_tokens:\n",
        "            # tokenize base_tokenizer\n",
        "            tokens = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
        "            valid_tokens = tokens.input_ids != base_tokenizer.pad_token_id\n",
        "            replace_pct = args.pct_words_masked * (args.span_length / (args.span_length + 2 * args.buffer_size))\n",
        "\n",
        "            # replace replace_pct of input_ids with random tokens\n",
        "            random_mask = torch.rand(tokens.input_ids.shape, device=DEVICE) < replace_pct\n",
        "            random_mask &= valid_tokens\n",
        "            random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
        "            # while any of the random tokens are special tokens, replace them with random non-special tokens\n",
        "            while any(base_tokenizer.decode(x) in base_tokenizer.all_special_tokens for x in random_tokens):\n",
        "                random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
        "            tokens.input_ids[random_mask] = random_tokens\n",
        "            perturbed_texts = base_tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=True)\n",
        "        else:\n",
        "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
        "            perturbed_texts = masked_texts\n",
        "            # replace each <extra_id_*> with args.span_length random words from FILL_DICTIONARY\n",
        "            for idx, text in enumerate(perturbed_texts):\n",
        "                filled_text = text\n",
        "                for fill_idx in range(count_masks([text])[0]):\n",
        "                    fill = random.sample(FILL_DICTIONARY, span_length)\n",
        "                    filled_text = filled_text.replace(f\"<extra_id_{fill_idx}>\", \" \".join(fill))\n",
        "                assert count_masks([filled_text])[0] == 0, \"Failed to replace all masks\"\n",
        "                perturbed_texts[idx] = filled_text\n",
        "\n",
        "    return perturbed_texts\n",
        "\n",
        "\n",
        "def perturb_texts(texts, span_length, pct, ceil_pct=False):\n",
        "    chunk_size = args.chunk_size\n",
        "    if '11b' in mask_filling_model_name:\n",
        "        chunk_size //= 2\n",
        "\n",
        "    outputs = []\n",
        "    for i in tqdm.tqdm(range(0, len(texts), chunk_size), desc=\"Applying perturbations\"):\n",
        "        outputs.extend(perturb_texts_(texts[i:i + chunk_size], span_length, pct, ceil_pct=ceil_pct))\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def drop_last_word(text):\n",
        "    return ' '.join(text.split(' ')[:-1])\n",
        "\n",
        "\n",
        "def get_likelihood(logits, labels):\n",
        "    assert logits.shape[0] == 1\n",
        "    assert labels.shape[0] == 1\n",
        "\n",
        "    logits = logits.view(-1, logits.shape[-1])[:-1]\n",
        "    labels = labels.view(-1)[1:]\n",
        "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    log_likelihood = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
        "    return log_likelihood.mean()\n",
        "\n",
        "\n",
        "# Get the log likelihood of each text under the base_model\n",
        "def get_ll(text):\n",
        "    if args.openai_model:\n",
        "        kwargs = { \"engine\": args.openai_model, \"temperature\": 0, \"max_tokens\": 0, \"echo\": True, \"logprobs\": 0}\n",
        "        r = openai.Completion.create(prompt=f\"<|endoftext|>{text}\", **kwargs)\n",
        "        result = r['choices'][0]\n",
        "        tokens, logprobs = result[\"logprobs\"][\"tokens\"][1:], result[\"logprobs\"][\"token_logprobs\"][1:]\n",
        "\n",
        "        assert len(tokens) == len(logprobs), f\"Expected {len(tokens)} logprobs, got {len(logprobs)}\"\n",
        "\n",
        "        return np.mean(logprobs)\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "            labels = tokenized.input_ids\n",
        "            return -base_model(**tokenized, labels=labels).loss.item()\n",
        "\n",
        "\n",
        "def get_lls(texts):\n",
        "    if not args.openai_model:\n",
        "        return [get_ll(text) for text in texts]\n",
        "    else:\n",
        "        global API_TOKEN_COUNTER\n",
        "\n",
        "        # use GPT2_TOKENIZER to get total number of tokens\n",
        "        total_tokens = sum(len(GPT2_TOKENIZER.encode(text)) for text in texts)\n",
        "        API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens\n",
        "\n",
        "        pool = ThreadPool(args.batch_size)\n",
        "        return pool.map(get_ll, texts)\n",
        "\n",
        "\n",
        "# get the average rank of each observed token sorted by model likelihood\n",
        "def get_rank(text, log=False):\n",
        "    assert args.openai_model is None, \"get_rank not implemented for OpenAI models\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "        logits = base_model(**tokenized).logits[:,:-1]\n",
        "        labels = tokenized.input_ids[:,1:]\n",
        "\n",
        "        # get rank of each label token in the model's likelihood ordering\n",
        "        matches = (logits.argsort(-1, descending=True) == labels.unsqueeze(-1)).nonzero()\n",
        "\n",
        "        assert matches.shape[1] == 3, f\"Expected 3 dimensions in matches tensor, got {matches.shape}\"\n",
        "\n",
        "        ranks, timesteps = matches[:,-1], matches[:,-2]\n",
        "\n",
        "        # make sure we got exactly one match for each timestep in the sequence\n",
        "        assert (timesteps == torch.arange(len(timesteps)).to(timesteps.device)).all(), \"Expected one match per timestep\"\n",
        "\n",
        "        ranks = ranks.float() + 1 # convert to 1-indexed rank\n",
        "        if log:\n",
        "            ranks = torch.log(ranks)\n",
        "\n",
        "        return ranks.float().mean().item()\n",
        "\n",
        "\n",
        "# get average entropy of each token in the text\n",
        "def get_entropy(text):\n",
        "    assert args.openai_model is None, \"get_entropy not implemented for OpenAI models\"\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "        logits = base_model(**tokenized).logits[:,:-1]\n",
        "        neg_entropy = F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)\n",
        "        return -neg_entropy.sum(-1).mean().item()\n",
        "\n",
        "\n",
        "def get_roc_metrics(real_preds, machine_preds):\n",
        "    fpr, tpr, _ = roc_curve([0] * len(real_preds) + [1] * len(machine_preds), real_preds + machine_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)\n",
        "\n",
        "\n",
        "def get_precision_recall_metrics(real_preds, machine_preds):\n",
        "    precision, recall, _ = precision_recall_curve([0] * len(real_preds) + [1] * len(machine_preds), real_preds + machine_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return precision.tolist(), recall.tolist(), float(pr_auc)\n",
        "\n",
        "\n",
        "# save the ROC curve for each experiment, given a list of output dictionaries, one for each experiment, using colorblind-friendly colors\n",
        "def save_roc_curves(experiments):\n",
        "    # first, clear plt\n",
        "    plt.clf()\n",
        "\n",
        "    for experiment, color in zip(experiments, COLORS):\n",
        "        metrics = experiment[\"metrics\"]\n",
        "        plt.plot(metrics[\"fpr\"], metrics[\"tpr\"], label=f\"{experiment['name']}, roc_auc={metrics['roc_auc']:.3f}\", color=color)\n",
        "        # print roc_auc for this experiment\n",
        "        print(f\"{experiment['name']} roc_auc: {metrics['roc_auc']:.3f}\")\n",
        "    plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curves ({base_model_name} - {args.mask_filling_model_name})')\n",
        "    plt.legend(loc=\"lower right\", fontsize=6)\n",
        "    plt.savefig(f\"{SAVE_FOLDER}/roc_curves.png\")\n",
        "\n",
        "\n",
        "# save the histogram of log likelihoods in two side-by-side plots, one for real and real perturbed, and one for machine and machine perturbed\n",
        "def save_ll_histograms(experiments):\n",
        "    # first, clear plt\n",
        "    plt.clf()\n",
        "\n",
        "    for experiment in experiments:\n",
        "        try:\n",
        "            results = experiment[\"raw_results\"]\n",
        "            # plot histogram of machine/perturbed machine on left, human/perturbed human on right\n",
        "            plt.figure(figsize=(20, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.hist([r[\"machine_ll\"] for r in results], alpha=0.5, bins='auto', label='machine')\n",
        "            plt.hist([r[\"perturbed_machine_ll\"] for r in results], alpha=0.5, bins='auto', label='perturbed machine')\n",
        "            plt.xlabel(\"log likelihood\")\n",
        "            plt.ylabel('count')\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.hist([r[\"human_ll\"] for r in results], alpha=0.5, bins='auto', label='human')\n",
        "            plt.hist([r[\"perturbed_human_ll\"] for r in results], alpha=0.5, bins='auto', label='perturbed human')\n",
        "            plt.xlabel(\"log likelihood\")\n",
        "            plt.ylabel('count')\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.savefig(f\"{SAVE_FOLDER}/ll_histograms_{experiment['name']}.png\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "# save the histograms of log likelihood ratios in two side-by-side plots, one for real and real perturbed, and one for machine and machine perturbed\n",
        "def save_llr_histograms(experiments):\n",
        "    # first, clear plt\n",
        "    plt.clf()\n",
        "\n",
        "    for experiment in experiments:\n",
        "        try:\n",
        "            results = experiment[\"raw_results\"]\n",
        "            # plot histogram of machine/perturbed machine on left, human/perturbed human on right\n",
        "            plt.figure(figsize=(20, 6))\n",
        "            plt.subplot(1, 2, 1)\n",
        "\n",
        "            # compute the log likelihood ratio for each result\n",
        "            for r in results:\n",
        "                r[\"machine_llr\"] = r[\"machine_ll\"] - r[\"perturbed_machine_ll\"]\n",
        "                r[\"human_llr\"] = r[\"human_ll\"] - r[\"perturbed_human_ll\"]\n",
        "\n",
        "            plt.hist([r[\"machine_llr\"] for r in results], alpha=0.5, bins='auto', label='machine')\n",
        "            plt.hist([r[\"human_llr\"] for r in results], alpha=0.5, bins='auto', label='human')\n",
        "            plt.xlabel(\"log likelihood ratio\")\n",
        "            plt.ylabel('count')\n",
        "            plt.legend(loc='upper right')\n",
        "            plt.savefig(f\"{SAVE_FOLDER}/llr_histograms_{experiment['name']}.png\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "def get_perturbation_results(span_length=10, n_perturbations=1, n_samples=500):\n",
        "    load_mask_model()\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    results = []\n",
        "    human_text = data[\"human\"]\n",
        "    machine_text = data[\"machine\"]\n",
        "\n",
        "    perturb_fn = functools.partial(perturb_texts, span_length=span_length, pct=args.pct_words_masked)\n",
        "\n",
        "    p_human_text = perturb_fn([x for x in human_text for _ in range(n_perturbations)])\n",
        "    p_machine_text = perturb_fn([x for x in machine_text for _ in range(n_perturbations)])\n",
        "    for _ in range(n_perturbation_rounds - 1):\n",
        "        try:\n",
        "            p_machine_text, p_human_text = perturb_fn(p_machine_text), perturb_fn(p_human_text)\n",
        "        except AssertionError:\n",
        "            break\n",
        "\n",
        "    assert len(p_machine_text) == len(machine_text) * n_perturbations, f\"Expected {len(machine_text) * n_perturbations} perturbed samples, got {len(p_machine_text)}\"\n",
        "    assert len(p_human_text) == len(human_text) * n_perturbations, f\"Expected {len(human_text) * n_perturbations} perturbed samples, got {len(p_human_text)}\"\n",
        "\n",
        "    for idx in range(len(human_text)):\n",
        "        results.append({\n",
        "            \"human\": human_text[idx],\n",
        "            \"machine\": machine_text[idx],\n",
        "            \"perturbed_machine\": p_machine_text[idx * n_perturbations: (idx + 1) * n_perturbations],\n",
        "            \"perturbed_human\": p_human_text[idx * n_perturbations: (idx + 1) * n_perturbations]\n",
        "        })\n",
        "\n",
        "    load_base_model()\n",
        "\n",
        "    for res in tqdm.tqdm(results, desc=\"Computing log likelihoods\"):\n",
        "        p_machine_ll = get_lls(res[\"perturbed_machine\"])\n",
        "        p_human_ll = get_lls(res[\"perturbed_human\"])\n",
        "        res[\"human_ll\"] = get_ll(res[\"human\"])\n",
        "        res[\"machine_ll\"] = get_ll(res[\"machine\"])\n",
        "        res[\"all_perturbed_machine_ll\"] = p_machine_ll\n",
        "        res[\"all_perturbed_human_ll\"] = p_human_ll\n",
        "        res[\"perturbed_machine_ll\"] = np.mean(p_machine_ll)\n",
        "        res[\"perturbed_human_ll\"] = np.mean(p_human_ll)\n",
        "        res[\"perturbed_machine_ll_std\"] = np.std(p_machine_ll) if len(p_machine_ll) > 1 else 1\n",
        "        res[\"perturbed_human_ll_std\"] = np.std(p_human_ll) if len(p_human_ll) > 1 else 1\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_perturbation_experiment(results, criterion, span_length=10, n_perturbations=1, n_samples=500):\n",
        "    # compute diffs with perturbed\n",
        "    predictions = {'real': [], 'machine': []}\n",
        "    for res in results:\n",
        "        if criterion == 'd':\n",
        "            predictions['real'].append(res['human_ll'] - res['perturbed_human_ll'])\n",
        "            predictions['machine'].append(res['machine_ll'] - res['perturbed_machine_ll'])\n",
        "        elif criterion == 'z':\n",
        "            if res['perturbed_human_ll_std'] == 0:\n",
        "                res['perturbed_human_ll_std'] = 1\n",
        "                print(\"WARNING: std of perturbed human is 0, setting to 1\")\n",
        "                print(f\"Number of unique perturbed human texts: {len(set(res['perturbed_human']))}\")\n",
        "                print(f\"human text: {res['human']}\")\n",
        "            if res['perturbed_machine_ll_std'] == 0:\n",
        "                res['perturbed_machine_ll_std'] = 1\n",
        "                print(\"WARNING: std of perturbed machine is 0, setting to 1\")\n",
        "                print(f\"Number of unique perturbed machine texts: {len(set(res['perturbed_machine']))}\")\n",
        "                print(f\"machine text: {res['machine']}\")\n",
        "            predictions['real'].append((res['human_ll'] - res['perturbed_human_ll']) / res['perturbed_human_ll_std'])\n",
        "            predictions['machine'].append((res['machine_ll'] - res['perturbed_machine_ll']) / res['perturbed_machine_ll_std'])\n",
        "\n",
        "    print(predictions['real'], predictions['machine'])\n",
        "    fpr, tpr, roc_auc = get_roc_metrics(predictions['real'], predictions['machine'])\n",
        "    p, r, pr_auc = get_precision_recall_metrics(predictions['real'], predictions['machine'])\n",
        "    name = f'perturbation_{n_perturbations}_{criterion}'\n",
        "    print(f\"{name} ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")\n",
        "    return {\n",
        "        'name': name,\n",
        "        'predictions': predictions,\n",
        "        'info': {\n",
        "            'pct_words_masked': args.pct_words_masked,\n",
        "            'span_length': span_length,\n",
        "            'n_perturbations': n_perturbations,\n",
        "            'n_samples': n_samples,\n",
        "        },\n",
        "        'raw_results': results,\n",
        "        'metrics': {\n",
        "            'roc_auc': roc_auc,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "        },\n",
        "        'pr_metrics': {\n",
        "            'pr_auc': pr_auc,\n",
        "            'precision': p,\n",
        "            'recall': r,\n",
        "        },\n",
        "        'loss': 1 - pr_auc,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_baseline_threshold_experiment(criterion_fn, name, n_samples=500):\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "    n_samples = min(len(data[\"human\"]),len(data[\"machine\"]))\n",
        "    results = []\n",
        "    for batch in tqdm.tqdm(range(n_samples // batch_size), desc=f\"Computing {name} criterion\"):\n",
        "        human_text = data[\"human\"][batch * batch_size:(batch + 1) * batch_size]\n",
        "        machine_text = data[\"machine\"][batch * batch_size:(batch + 1) * batch_size]\n",
        "\n",
        "        for idx in range(len(human_text)):\n",
        "            results.append({\n",
        "                \"human\": human_text[idx],\n",
        "                \"human_crit\": criterion_fn(human_text[idx]),\n",
        "                \"machine\": machine_text[idx],\n",
        "                \"machine_crit\": criterion_fn(machine_text[idx]),\n",
        "            })\n",
        "\n",
        "    # compute prediction scores for real/machine passages\n",
        "    predictions = {\n",
        "        'real': [x[\"human_crit\"] for x in results],\n",
        "        'machine': [x[\"machine_crit\"] for x in results],\n",
        "    }\n",
        "\n",
        "    fpr, tpr, roc_auc = get_roc_metrics(predictions['real'], predictions['machine'])\n",
        "    p, r, pr_auc = get_precision_recall_metrics(predictions['real'], predictions['machine'])\n",
        "    print(f\"{name}_threshold ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")\n",
        "    return {\n",
        "        'name': f'{name}_threshold',\n",
        "        'predictions': predictions,\n",
        "        'info': {\n",
        "            'n_samples': n_samples,\n",
        "        },\n",
        "        'raw_results': results,\n",
        "        'metrics': {\n",
        "            'roc_auc': roc_auc,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "        },\n",
        "        'pr_metrics': {\n",
        "            'pr_auc': pr_auc,\n",
        "            'precision': p,\n",
        "            'recall': r,\n",
        "        },\n",
        "        'loss': 1 - pr_auc,\n",
        "    }\n",
        "\n",
        "\n",
        "# strip newlines from each example; replace one or more newlines with a single space\n",
        "def strip_newlines(text):\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "\n",
        "# trim to shorter length\n",
        "def trim_to_shorter_length(texta, textb):\n",
        "    # truncate to shorter of o and s\n",
        "    shorter_length = min(len(texta.split(' ')), len(textb.split(' ')))\n",
        "    texta = ' '.join(texta.split(' ')[:shorter_length])\n",
        "    textb = ' '.join(textb.split(' ')[:shorter_length])\n",
        "    return texta, textb\n",
        "\n",
        "\n",
        "def truncate_to_substring(text, substring, idx_occurrence):\n",
        "    # truncate everything after the idx_occurrence occurrence of substring\n",
        "    assert idx_occurrence > 0, 'idx_occurrence must be > 0'\n",
        "    idx = -1\n",
        "    for _ in range(idx_occurrence):\n",
        "        idx = text.find(substring, idx + 1)\n",
        "        if idx == -1:\n",
        "            return text\n",
        "    return text[:idx]\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    text_1 = strip_newlines(text.strip())\n",
        "    if len(preproc_tokenizer(text_1)[\"input_ids\"]) <= 512:\n",
        "        return text_1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def load_data(dataset, key):\n",
        "\n",
        "    jsonl_file_path = script_directory+'/SubtaskA/subtaskA_dev_monolingual.jsonl'\n",
        "    data_all = pd.read_json(jsonl_file_path, lines=True)\n",
        "    data = {\n",
        "        \"human\": [],\n",
        "        \"machine\": [],\n",
        "      }\n",
        "    for index, row in data_all.iterrows():\n",
        "        text = preprocess_text(row['text'])\n",
        "        if text:\n",
        "            if row['label'] == 0:\n",
        "                data['human'].append(preprocess_text(row['text']))\n",
        "            else:\n",
        "                data['machine'].append(preprocess_text(row['text']))\n",
        "\n",
        "\n",
        "    # Remove duplicates from each list\n",
        "    data[\"human\"] = list(set(data[\"human\"]))\n",
        "    data[\"machine\"] = list(set(data[\"machine\"]))\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def load_base_model_and_tokenizer(name):\n",
        "    if args.openai_model is None:\n",
        "        print(f'Loading BASE model {args.base_model_name}...')\n",
        "        base_model_kwargs = {}\n",
        "        if 'gpt-j' in name or 'neox' in name:\n",
        "            base_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
        "        if 'gpt-j' in name:\n",
        "            base_model_kwargs.update(dict(revision='float16'))\n",
        "        base_model = transformers.AutoModelForCausalLM.from_pretrained(name, **base_model_kwargs, cache_dir=cache_dir)\n",
        "    else:\n",
        "        base_model = None\n",
        "\n",
        "    optional_tok_kwargs = {}\n",
        "    if \"facebook/opt-\" in name:\n",
        "        print(\"Using non-fast tokenizer for OPT\")\n",
        "        optional_tok_kwargs['fast'] = False\n",
        "    if args.dataset in ['pubmed']:\n",
        "        optional_tok_kwargs['padding_side'] = 'left'\n",
        "    base_tokenizer = transformers.AutoTokenizer.from_pretrained(name, **optional_tok_kwargs, cache_dir=cache_dir)\n",
        "    base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
        "\n",
        "    return base_model, base_tokenizer\n",
        "\n",
        "\n",
        "def eval_supervised(data, model):\n",
        "    print(f'Beginning supervised evaluation with {model}...')\n",
        "    detector = transformers.AutoModelForSequenceClassification.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
        "    tokenizer = transformers.AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
        "\n",
        "    real, fake = data['human'], data['machine']\n",
        "    with torch.no_grad():\n",
        "        # get predictions for real\n",
        "        real_preds = []\n",
        "        for batch in tqdm.tqdm(range(len(real) // batch_size), desc=\"Evaluating real\"):\n",
        "            batch_real = real[batch * batch_size:(batch + 1) * batch_size]\n",
        "            batch_real = tokenizer(batch_real, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
        "            real_preds.extend(detector(**batch_real).logits.softmax(-1)[:,0].tolist())\n",
        "\n",
        "        # get predictions for fake\n",
        "        fake_preds = []\n",
        "        for batch in tqdm.tqdm(range(len(fake) // batch_size), desc=\"Evaluating fake\"):\n",
        "            batch_fake = fake[batch * batch_size:(batch + 1) * batch_size]\n",
        "            batch_fake = tokenizer(batch_fake, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
        "            fake_preds.extend(detector(**batch_fake).logits.softmax(-1)[:,0].tolist())\n",
        "\n",
        "    predictions = {\n",
        "        'real': real_preds,\n",
        "        'machine': fake_preds,\n",
        "    }\n",
        "\n",
        "    fpr, tpr, roc_auc = get_roc_metrics(real_preds, fake_preds)\n",
        "    p, r, pr_auc = get_precision_recall_metrics(real_preds, fake_preds)\n",
        "    print(f\"{model} ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")\n",
        "\n",
        "    # free GPU memory\n",
        "    del detector\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return {\n",
        "        'name': model,\n",
        "        'predictions': predictions,\n",
        "        'info': {\n",
        "            'n_samples': n_samples,\n",
        "        },\n",
        "        'metrics': {\n",
        "            'roc_auc': roc_auc,\n",
        "            'fpr': fpr,\n",
        "            'tpr': tpr,\n",
        "        },\n",
        "        'pr_metrics': {\n",
        "            'pr_auc': pr_auc,\n",
        "            'precision': p,\n",
        "            'recall': r,\n",
        "        },\n",
        "        'loss': 1 - pr_auc,\n",
        "    }\n",
        "\n",
        "def check_dataset():\n",
        "\n",
        "    jsonl_file_path = os.path.join(script_directory, 'SubtaskA', 'subtaskA_dev_monolingual.jsonl')\n",
        "    if not os.path.isfile(jsonl_file_path):\n",
        "        print(f\"Error: File '{jsonl_file_path}' not found.\")\n",
        "        return False\n",
        "\n",
        "    # Try to load the JSONL file\n",
        "    try:\n",
        "        with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
        "            # Assuming each line in the JSONL file is a valid JSON object\n",
        "            json_data = [json.loads(line) for line in file]\n",
        "        print(f\"Dataset loaded successfully from '{jsonl_file_path}'.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from '{jsonl_file_path}': {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def download_dataset():\n",
        "\n",
        "  # Retrieving the data from the public folders\n",
        "  %cd /content/drive/My Drive/\n",
        "  !gdown --folder https://drive.google.com/drive/folders/1CAbb3DjrOPBNm0ozVBfhvrEh9P9rAppc\n",
        "\n"
      ],
      "metadata": {
        "id": "Z2LJQgxeWccb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the directory of the current script\n",
        "script_directory = r'/content/drive/My Drive/'\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    args = DotMap({\n",
        "        'dataset': \"m4\",\n",
        "        'dataset_key': \"key\",\n",
        "        'pct_words_masked': 0.3,\n",
        "        'span_length': 2,\n",
        "        'n_samples': 200,\n",
        "        'n_perturbation_list': \"1,10\",\n",
        "        'n_perturbation_rounds': 1,\n",
        "        'base_model_name': \"gpt2-medium\",\n",
        "        'scoring_model_name': \"\",\n",
        "        'mask_filling_model_name': \"t5-large\",\n",
        "        'batch_size': 50,\n",
        "        'chunk_size': 20,\n",
        "        'n_similarity_samples': 20,\n",
        "        'int8': False,\n",
        "        'half': False,\n",
        "        'base_half': False,\n",
        "        'do_top_k': False,\n",
        "        'top_k': 40,\n",
        "        'do_top_p': False,\n",
        "        'top_p': 0.96,\n",
        "        'output_name': \"\",\n",
        "        'openai_model': None,\n",
        "        'openai_key': None,\n",
        "        'baselines_only': True,\n",
        "        'skip_baselines': False,\n",
        "        'buffer_size': 1,\n",
        "        'mask_top_p': 1.0,\n",
        "        'pre_perturb_pct': 0.0,\n",
        "        'pre_perturb_span_length': 5,\n",
        "        'random_fills': False,\n",
        "        'random_fills_tokens': False\n",
        "    })\n",
        "\n",
        "    DEVICE = \"cuda\"\n",
        "\n",
        "    API_TOKEN_COUNTER = 0\n",
        "\n",
        "\n",
        "\n",
        "    if not check_dataset():\n",
        "        download_dataset()\n",
        "\n",
        "    if args.openai_model is not None:\n",
        "        import openai\n",
        "        assert args.openai_key is not None, \"Must provide OpenAI API key as --openai_key\"\n",
        "        openai.api_key = args.openai_key\n",
        "\n",
        "    START_DATE = datetime.datetime.now().strftime('%Y-%m-%d')\n",
        "    START_TIME = datetime.datetime.now().strftime('%H-%M-%S-%f')\n",
        "\n",
        "    # define SAVE_FOLDER as the timestamp - base model name - mask filling model name\n",
        "    # create it if it doesn't exist\n",
        "    precision_string = \"int8\" if args.int8 else (\"fp16\" if args.half else \"fp32\")\n",
        "    sampling_string = \"top_k\" if args.do_top_k else (\"top_p\" if args.do_top_p else \"temp\")\n",
        "    output_subfolder = f\"{args.output_name}/\" if args.output_name else \"\"\n",
        "    if args.openai_model is None:\n",
        "        base_model_name = args.base_model_name.replace('/', '_')\n",
        "    else:\n",
        "        base_model_name = \"openai-\" + args.openai_model.replace('/', '_')\n",
        "    scoring_model_string = (f\"-{args.scoring_model_name}\" if args.scoring_model_name else \"\").replace('/', '_')\n",
        "    SAVE_FOLDER = script_directory+f\"/tmp_results/{output_subfolder}{base_model_name}{scoring_model_string}-{args.mask_filling_model_name}-{sampling_string}/{START_DATE}-{START_TIME}-{precision_string}-{args.pct_words_masked}-{args.n_perturbation_rounds}-{args.dataset}-{args.n_samples}\"\n",
        "    if not os.path.exists(SAVE_FOLDER):\n",
        "        os.makedirs(SAVE_FOLDER)\n",
        "    print(f\"Saving results to absolute path: {os.path.abspath(SAVE_FOLDER)}\")\n",
        "\n",
        "    # write args to file\n",
        "    with open(os.path.join(SAVE_FOLDER, \"args.json\"), \"w\") as f:\n",
        "        json.dump(args.__dict__, f, indent=4)\n",
        "\n",
        "    mask_filling_model_name = args.mask_filling_model_name\n",
        "    n_samples = args.n_samples\n",
        "    batch_size = args.batch_size\n",
        "    n_perturbation_list = [int(x) for x in args.n_perturbation_list.split(\",\")]\n",
        "    n_perturbation_rounds = args.n_perturbation_rounds\n",
        "    n_similarity_samples = args.n_similarity_samples\n",
        "\n",
        "    cache_dir = script_directory + '/cache'\n",
        "    os.environ[\"XDG_CACHE_HOME\"] = cache_dir\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "    print(f\"Using cache dir {cache_dir}\")\n",
        "\n",
        "    GPT2_TOKENIZER = transformers.GPT2Tokenizer.from_pretrained('gpt2', cache_dir=cache_dir)\n",
        "\n",
        "    # generic generative model\n",
        "    base_model, base_tokenizer = load_base_model_and_tokenizer(args.base_model_name)\n",
        "\n",
        "    # mask filling t5 model\n",
        "    if not args.baselines_only and not args.random_fills:\n",
        "        int8_kwargs = {}\n",
        "        half_kwargs = {}\n",
        "        if args.int8:\n",
        "            int8_kwargs = dict(load_in_8bit=True, device_map='auto', torch_dtype=torch.bfloat16)\n",
        "        elif args.half:\n",
        "            half_kwargs = dict(torch_dtype=torch.bfloat16)\n",
        "        print(f'Loading mask filling model {mask_filling_model_name}...')\n",
        "        mask_model = transformers.AutoModelForSeq2SeqLM.from_pretrained(mask_filling_model_name, **int8_kwargs, **half_kwargs, cache_dir=cache_dir)\n",
        "        try:\n",
        "            n_positions = mask_model.config.n_positions\n",
        "        except AttributeError:\n",
        "            n_positions = 512\n",
        "    else:\n",
        "        n_positions = 512\n",
        "    preproc_tokenizer = transformers.AutoTokenizer.from_pretrained('t5-small', model_max_length=512, cache_dir=cache_dir)\n",
        "    mask_tokenizer = transformers.AutoTokenizer.from_pretrained(mask_filling_model_name, model_max_length=n_positions, cache_dir=cache_dir)\n",
        "    if args.dataset in ['english', 'german']:\n",
        "        preproc_tokenizer = mask_tokenizer\n",
        "\n",
        "    load_base_model()\n",
        "\n",
        "    print(f'Loading dataset {args.dataset}...')\n",
        "    data = load_data(args.dataset, args.dataset_key)\n",
        "    if args.random_fills:\n",
        "        FILL_DICTIONARY = set()\n",
        "        for texts in data.values():\n",
        "            for text in texts:\n",
        "                FILL_DICTIONARY.update(text.split())\n",
        "        FILL_DICTIONARY = sorted(list(FILL_DICTIONARY))\n",
        "\n",
        "    if args.scoring_model_name:\n",
        "        print(f'Loading SCORING model {args.scoring_model_name}...')\n",
        "        del base_model\n",
        "        del base_tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "        base_model, base_tokenizer = load_base_model_and_tokenizer(args.scoring_model_name)\n",
        "        load_base_model()  # Load again because we've deleted/replaced the old model\n",
        "\n",
        "    # write the data to a json file in the save folder\n",
        "    with open(os.path.join(SAVE_FOLDER, \"raw_data.json\"), \"w\") as f:\n",
        "        print(f\"Writing raw data to {os.path.join(SAVE_FOLDER, 'raw_data.json')}\")\n",
        "        json.dump(data, f)\n",
        "\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    if not args.skip_baselines:\n",
        "        baseline_outputs = [run_baseline_threshold_experiment(get_ll, \"likelihood\", n_samples=n_samples)]\n",
        "        if args.openai_model is None:\n",
        "            rank_criterion = lambda text: -get_rank(text, log=False)\n",
        "            baseline_outputs.append(run_baseline_threshold_experiment(rank_criterion, \"rank\", n_samples=n_samples))\n",
        "            logrank_criterion = lambda text: -get_rank(text, log=True)\n",
        "            baseline_outputs.append(run_baseline_threshold_experiment(logrank_criterion, \"log_rank\", n_samples=n_samples))\n",
        "            entropy_criterion = lambda text: get_entropy(text)\n",
        "            baseline_outputs.append(run_baseline_threshold_experiment(entropy_criterion, \"entropy\", n_samples=n_samples))\n",
        "\n",
        "        baseline_outputs.append(eval_supervised(data, model='roberta-base-openai-detector'))\n",
        "        baseline_outputs.append(eval_supervised(data, model='roberta-large-openai-detector'))\n",
        "\n",
        "        # write likelihood threshold results to a file\n",
        "        with open(os.path.join(SAVE_FOLDER, f\"likelihood_threshold_results.json\"), \"w\") as f:\n",
        "            json.dump(baseline_outputs[0], f)\n",
        "\n",
        "        if args.openai_model is None:\n",
        "            # write rank threshold results to a file\n",
        "            with open(os.path.join(SAVE_FOLDER, f\"rank_threshold_results.json\"), \"w\") as f:\n",
        "                json.dump(baseline_outputs[1], f)\n",
        "\n",
        "            # write log rank threshold results to a file\n",
        "            with open(os.path.join(SAVE_FOLDER, f\"logrank_threshold_results.json\"), \"w\") as f:\n",
        "                json.dump(baseline_outputs[2], f)\n",
        "\n",
        "            # write entropy threshold results to a file\n",
        "            with open(os.path.join(SAVE_FOLDER, f\"entropy_threshold_results.json\"), \"w\") as f:\n",
        "                json.dump(baseline_outputs[3], f)\n",
        "\n",
        "        # write supervised results to a file\n",
        "        with open(os.path.join(SAVE_FOLDER, f\"roberta-base-openai-detector_results.json\"), \"w\") as f:\n",
        "            json.dump(baseline_outputs[-2], f)\n",
        "\n",
        "        # write supervised results to a file\n",
        "        with open(os.path.join(SAVE_FOLDER, f\"roberta-large-openai-detector_results.json\"), \"w\") as f:\n",
        "            json.dump(baseline_outputs[-1], f)\n",
        "\n",
        "        outputs += baseline_outputs\n",
        "\n",
        "    if not args.baselines_only:\n",
        "        # run perturbation experiments\n",
        "        for n_perturbations in n_perturbation_list:\n",
        "            perturbation_results = get_perturbation_results(args.span_length, n_perturbations, n_samples)\n",
        "            for perturbation_mode in ['d', 'z']:\n",
        "                output = run_perturbation_experiment(\n",
        "                    perturbation_results, perturbation_mode, span_length=args.span_length, n_perturbations=n_perturbations, n_samples=n_samples)\n",
        "                outputs.append(output)\n",
        "                with open(os.path.join(SAVE_FOLDER, f\"perturbation_{n_perturbations}_{perturbation_mode}_results.json\"), \"w\") as f:\n",
        "                    json.dump(output, f)\n",
        "\n",
        "    save_roc_curves(outputs)\n",
        "    save_ll_histograms(outputs)\n",
        "    save_llr_histograms(outputs)\n",
        "\n",
        "    # move results folder from tmp_results/ to results/, making sure necessary directories exist\n",
        "    new_folder = SAVE_FOLDER.replace(\"tmp_results\", \"results\")\n",
        "    if not os.path.exists(os.path.dirname(new_folder)):\n",
        "        os.makedirs(os.path.dirname(new_folder))\n",
        "    os.rename(SAVE_FOLDER, new_folder)\n",
        "\n",
        "    print(f\"Used an *estimated* {API_TOKEN_COUNTER} API tokens (may be inaccurate)\")"
      ],
      "metadata": {
        "id": "vSzgViQqZ9G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_roc_metrics(predictions_dict):\n",
        "    real_preds = predictions_dict['real']\n",
        "    machine_preds = predictions_dict['machine']\n",
        "    fpr, tpr, _ = roc_curve([0] * len(real_preds) + [1] * len(machine_preds), real_preds + machine_preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    return fpr.tolist(), tpr.tolist(), float(roc_auc)\n",
        "\n",
        "def get_precision_recall_metrics(predictions_dict):\n",
        "    real_preds = predictions_dict['real']\n",
        "    machine_preds = predictions_dict['machine']\n",
        "    precision, recall, _ = precision_recall_curve([0] * len(real_preds) + [1] * len(machine_preds), real_preds + machine_preds)\n",
        "    pr_auc = auc(recall, precision)\n",
        "    return precision.tolist(), recall.tolist(), float(pr_auc)\n",
        "\n",
        "def get_accuracy_f1_metrics(predictions_dict):\n",
        "\n",
        "    real_preds = predictions_dict['real']\n",
        "    machine_preds = predictions_dict['machine']\n",
        "    preds = real_preds + machine_preds\n",
        "    precision, recall, thresholds = precision_recall_curve([0] * len(real_preds) + [1] * len(machine_preds),preds )\n",
        "\n",
        "    # Calculate F1 score for each threshold\n",
        "    f1_scores = [f1_score([0] * len(real_preds) + [1] * len(machine_preds), [1 if score >= threshold else 0 for score in preds]) for threshold in thresholds]\n",
        "\n",
        "    # Find the index of the maximum F1 score\n",
        "    optimal_f1_threshold_index = f1_scores.index(max(f1_scores))\n",
        "    # Get the corresponding threshold\n",
        "    optimal_f1_threshold = thresholds[optimal_f1_threshold_index]\n",
        "    optimal_f1_score = f1_scores[optimal_f1_threshold_index]\n",
        "\n",
        "    # Calculate accuracy for each threshold\n",
        "    accuracies = [accuracy_score([0] * len(real_preds) + [1] * len(machine_preds), [1 if score >= threshold else 0 for score in preds]) for threshold in thresholds]\n",
        "\n",
        "    # Find the index of the maximum accuracy\n",
        "    optimal_accuracy_threshold_index = accuracies.index(max(accuracies))\n",
        "\n",
        "    # Get the corresponding threshold and accuracy\n",
        "    optimal_accuracy_threshold = thresholds[optimal_accuracy_threshold_index]\n",
        "    optimal_accuracy = accuracies[optimal_accuracy_threshold_index]\n",
        "\n",
        "\n",
        "    optimal_precision = precision_score([0] * len(real_preds) + [1] * len(machine_preds), [1 if score >= optimal_f1_threshold else 0 for score in preds])\n",
        "    optimal_recall = recall_score([0] * len(real_preds) + [1] * len(machine_preds), [1 if score >= optimal_f1_threshold else 0 for score in preds])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return optimal_accuracy, optimal_precision, optimal_recall, optimal_f1_score, optimal_accuracy_threshold, optimal_f1_threshold\n",
        "\n",
        "def plot_confusion_matrix(predictions_dict):\n",
        "\n",
        "\n",
        "    real_preds = predictions_dict['real']\n",
        "    machine_preds = predictions_dict['machine']\n",
        "\n",
        "    preds = real_preds + machine_preds\n",
        "    precision, recall, thresholds = precision_recall_curve([0] * len(real_preds) + [1] * len(machine_preds),preds )\n",
        "\n",
        "    # Calculate F1 score for each threshold\n",
        "    f1_scores = [f1_score([0] * len(real_preds) + [1] * len(machine_preds), [1 if score >= threshold else 0 for score in preds]) for threshold in thresholds]\n",
        "\n",
        "    # Find the index of the maximum F1 score\n",
        "    optimal_f1_threshold_index = f1_scores.index(max(f1_scores))\n",
        "    # Get the corresponding threshold\n",
        "    optimal_f1_threshold = thresholds[optimal_f1_threshold_index]\n",
        "\n",
        "    # Apply the threshold to get binary predictions\n",
        "    binary_predictions = (preds > optimal_f1_threshold).astype(int)\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    cm = confusion_matrix([0] * len(real_preds) + [1] * len(machine_preds), binary_predictions)\n",
        "\n",
        "    # Plot the confusion matrix\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "                xticklabels=['Human', 'Machine'], yticklabels=['Human', 'Machine'])\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.title(f'Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "def create_evaluation_df(data):\n",
        "\n",
        "  metric_data = {}\n",
        "  for key in data.keys():\n",
        "    metric_data[key] = {}\n",
        "    metric_data[key]['accuracy'], metric_data[key]['precision'], metric_data[key]['recall'], metric_data[key]['f1'],  _, _ = get_accuracy_f1_metrics(data[key]['predictions'])\n",
        "    metric_data[key]['roc_auc'] = data[key]['metrics']['roc_auc']\n",
        "\n",
        "  return pd.DataFrame.from_dict(metric_data, orient='index')\n",
        "\n",
        "def print_results(SAVE_FOLDER):\n",
        "\n",
        "  # Define the file names\n",
        "  file_names = [\n",
        "      \"rank_threshold_results.json\",\n",
        "      \"logrank_threshold_results.json\",\n",
        "      \"entropy_threshold_results.json\",\n",
        "      \"roberta-base-openai-detector_results.json\",\n",
        "      \"roberta-large-openai-detector_results.json\",\n",
        "      \"likelihood_threshold_results.json\",\n",
        "      \"perturbation_1_d_results.json\",\n",
        "      \"perturbation_1_z_results.json\",\n",
        "      \"perturbation_10_d_results.json\",\n",
        "      \"perturbation_10_z_results.json\"\n",
        "  ]\n",
        "\n",
        "  # Create a dictionary to store the data\n",
        "  data = {}\n",
        "\n",
        "  # Read each file and assign the data to variables\n",
        "  for file_name in file_names:\n",
        "      file_path = os.path.join(SAVE_FOLDER, file_name)\n",
        "\n",
        "      # Check if the file exists\n",
        "      if os.path.exists(file_path):\n",
        "          with open(file_path, \"r\") as f:\n",
        "              data[file_name.replace('_results.json', '')] = json.load(f)\n",
        "      else:\n",
        "          print(f\"The file {file_path} does not exist.\")\n",
        "  return data, create_evaluation_df(data)"
      ],
      "metadata": {
        "id": "79HJ9XJWARJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, evaluation_df = print_results('/content/drive/MyDrive/results/gpt2-medium-t5-large-temp/2023-12-07-00-04-08-643443-fp32-0.3-1-m4-200')"
      ],
      "metadata": {
        "id": "xVvVwZdYD2Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "pOu7ERRcROHq",
        "outputId": "07e5f50c-7e04-453f-b761-183277cdf80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8db817c7-aece-4ba7-a51b-37451edf78c0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>roc_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>rank_threshold</th>\n",
              "      <td>0.797500</td>\n",
              "      <td>0.761261</td>\n",
              "      <td>0.845000</td>\n",
              "      <td>0.800948</td>\n",
              "      <td>0.851950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>logrank_threshold</th>\n",
              "      <td>0.732500</td>\n",
              "      <td>0.676806</td>\n",
              "      <td>0.890000</td>\n",
              "      <td>0.768898</td>\n",
              "      <td>0.813325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>entropy_threshold</th>\n",
              "      <td>0.502500</td>\n",
              "      <td>0.501253</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.667780</td>\n",
              "      <td>0.197225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-base-openai-detector</th>\n",
              "      <td>0.620506</td>\n",
              "      <td>0.620471</td>\n",
              "      <td>0.999592</td>\n",
              "      <td>0.765671</td>\n",
              "      <td>0.489529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>roberta-large-openai-detector</th>\n",
              "      <td>0.624051</td>\n",
              "      <td>0.621381</td>\n",
              "      <td>0.998776</td>\n",
              "      <td>0.766124</td>\n",
              "      <td>0.461347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>likelihood_threshold</th>\n",
              "      <td>0.705000</td>\n",
              "      <td>0.636691</td>\n",
              "      <td>0.885000</td>\n",
              "      <td>0.740586</td>\n",
              "      <td>0.760750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perturbation_1_d</th>\n",
              "      <td>0.575131</td>\n",
              "      <td>0.506406</td>\n",
              "      <td>0.985564</td>\n",
              "      <td>0.669042</td>\n",
              "      <td>0.597746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perturbation_1_z</th>\n",
              "      <td>0.575131</td>\n",
              "      <td>0.506406</td>\n",
              "      <td>0.985564</td>\n",
              "      <td>0.669042</td>\n",
              "      <td>0.597746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perturbation_10_d</th>\n",
              "      <td>0.661745</td>\n",
              "      <td>0.573799</td>\n",
              "      <td>0.862205</td>\n",
              "      <td>0.689040</td>\n",
              "      <td>0.701595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perturbation_10_z</th>\n",
              "      <td>0.645013</td>\n",
              "      <td>0.555160</td>\n",
              "      <td>0.921260</td>\n",
              "      <td>0.692820</td>\n",
              "      <td>0.682782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8db817c7-aece-4ba7-a51b-37451edf78c0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8db817c7-aece-4ba7-a51b-37451edf78c0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8db817c7-aece-4ba7-a51b-37451edf78c0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-438803ee-5ce8-4869-9036-a887b1345f6a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-438803ee-5ce8-4869-9036-a887b1345f6a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-438803ee-5ce8-4869-9036-a887b1345f6a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                               accuracy  precision    recall        f1  \\\n",
              "rank_threshold                 0.797500   0.761261  0.845000  0.800948   \n",
              "logrank_threshold              0.732500   0.676806  0.890000  0.768898   \n",
              "entropy_threshold              0.502500   0.501253  1.000000  0.667780   \n",
              "roberta-base-openai-detector   0.620506   0.620471  0.999592  0.765671   \n",
              "roberta-large-openai-detector  0.624051   0.621381  0.998776  0.766124   \n",
              "likelihood_threshold           0.705000   0.636691  0.885000  0.740586   \n",
              "perturbation_1_d               0.575131   0.506406  0.985564  0.669042   \n",
              "perturbation_1_z               0.575131   0.506406  0.985564  0.669042   \n",
              "perturbation_10_d              0.661745   0.573799  0.862205  0.689040   \n",
              "perturbation_10_z              0.645013   0.555160  0.921260  0.692820   \n",
              "\n",
              "                                roc_auc  \n",
              "rank_threshold                 0.851950  \n",
              "logrank_threshold              0.813325  \n",
              "entropy_threshold              0.197225  \n",
              "roberta-base-openai-detector   0.489529  \n",
              "roberta-large-openai-detector  0.461347  \n",
              "likelihood_threshold           0.760750  \n",
              "perturbation_1_d               0.597746  \n",
              "perturbation_1_z               0.597746  \n",
              "perturbation_10_d              0.701595  \n",
              "perturbation_10_z              0.682782  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(data['rank_threshold']['predictions'])"
      ],
      "metadata": {
        "id": "4N7Vrth4SQpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(data['perturbation_10_d']['predictions'])"
      ],
      "metadata": {
        "id": "bum8sCEpROlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}